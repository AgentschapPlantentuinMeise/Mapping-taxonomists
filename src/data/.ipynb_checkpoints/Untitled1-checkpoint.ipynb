{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83082319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6aaad04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melanie\\AppData\\Local\\Temp\\ipykernel_12256\\2542485849.py:12: DtypeWarning: Columns (9,10,16,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  backbone = pd.read_csv(\"../../data/external/backbone/Taxon.tsv\", sep=\"\\t\", on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "authors = pd.read_pickle(\"../../data/interim/european_taxonomic_authors_no_duplicates.pkl\")\n",
    "# less columns and author ID as index quicken processing\n",
    "authors = authors[[\"author_id\", \"author_display_name\", \"orcid\",\n",
    "                   \"inst_id\", \"inst_display_name\",  \"species_subject\"]]\n",
    "authors = authors.set_index(\"author_id\")\n",
    "\n",
    "levels = [\"genus\", \"family\", \"order\", \"class\", \"phylum\", \"kingdom\"]\n",
    "for level in levels:\n",
    "    authors[level] = [list() for x in range(len(authors.index))]\n",
    "\n",
    "# GBIF taxonomic backbone\n",
    "backbone = pd.read_csv(\"../../data/external/backbone/Taxon.tsv\", sep=\"\\t\", on_bad_lines='skip')\n",
    "\n",
    "# reduce size of backbone for easier searching\n",
    "backbone = backbone[backbone[\"taxonomicStatus\"]!=\"doubtful\"]\n",
    "backbone = backbone[[\"canonicalName\",] + levels]\n",
    "# remove taxa with no known species name, genus, family,...\n",
    "backbone = backbone.dropna().drop_duplicates(ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a96b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get initial first name + last name for every author\n",
    "truncated_names = []\n",
    "\n",
    "for author in authors.itertuples():\n",
    "    first_initial = author.author_display_name[0]\n",
    "    last_name = author.author_display_name.split(\" \")[-1]\n",
    "    truncated_names.append(first_initial + \" \" + last_name)\n",
    "    \n",
    "authors[\"truncatedName\"] = truncated_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015d8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone to dictionary for quicker processing\n",
    "seen_species = {}\n",
    "\n",
    "for species in backbone.itertuples():\n",
    "    if species.canonicalName not in seen_species:\n",
    "        seen_species[species.canonicalName] = list(species)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec81bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every author, break down each species associated with them into genus, family,... etc\n",
    "for i, author in authors.iterrows():\n",
    "    for species in author[\"species_subject\"]:\n",
    "        if species in seen_species:\n",
    "            # taxonomic levels such as genus, family\n",
    "            for l, level in enumerate(levels):\n",
    "                taxon_name = seen_species[species][l]\n",
    "                # make sure there are no duplicates\n",
    "                if taxon_name not in author[level]:\n",
    "                    authors.loc[i, level].append(taxon_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca015145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(a, b):\n",
    "    same = False\n",
    "    # if no known orders for one of them, just use institution \n",
    "    if a.order == [] or b.order == []:\n",
    "        if a.inst_id == b.inst_id:\n",
    "            same = True\n",
    "    # if both have known orders, orders and institution must match\n",
    "    else:\n",
    "        if a.inst_id == b.inst_id and len(set(a.order).intersection(set(b.order))) > 0:\n",
    "            same = True\n",
    "    \n",
    "    return same\n",
    "\n",
    "\n",
    "def cluster(matches):\n",
    "    clusters = []\n",
    "    \n",
    "    for match in matches:\n",
    "        # first list\n",
    "        if len(clusters) == 0:\n",
    "            clusters.append(match)\n",
    "        # check if it matches any existing groups\n",
    "        match_with_groups = []\n",
    "        for i, group in enumerate(clusters):\n",
    "            if len(set(match).intersection(set(group))) > 0:\n",
    "                match_with_groups.append(i)\n",
    "        print(match_with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e70acc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(a, b):\n",
    "    same = False\n",
    "    # if no known orders for one of them, just use institution \n",
    "    if a.order == [] or b.order == []:\n",
    "        if a.inst_id == b.inst_id:\n",
    "            same = True\n",
    "    # if both have known orders, orders and institution must match\n",
    "    else:\n",
    "        if a.inst_id == b.inst_id and len(set(a.order).intersection(set(b.order))) > 0:\n",
    "            same = True\n",
    "    \n",
    "    return same\n",
    "\n",
    "\n",
    "def cluster(matches):\n",
    "    clusters = []\n",
    "    \n",
    "    for match in matches:\n",
    "        # check if it matches any existing groups\n",
    "        match_with_groups = []\n",
    "        for i, group in enumerate(clusters):\n",
    "            if len(set(match).intersection(set(group))) > 0:\n",
    "                match_with_groups.append(i)\n",
    "        \n",
    "        # if it fits with no existing group, add it by itself\n",
    "        if len(match_with_groups) == 0:\n",
    "            clusters.append(match)\n",
    "        # if it fits with one existing group, add it to that group\n",
    "        elif len(match_with_groups) == 1:\n",
    "            clusters[match_with_groups[0]].extend(match)\n",
    "        # if it fits with multiple groups, mash those groups together\n",
    "        else:\n",
    "            print(clusters) # apparently this never happens\n",
    "            supergroup = []\n",
    "            # remove each group and add its contents to the new supergroup\n",
    "            for j in match_with_groups.sort(reverse=True):\n",
    "                supergroup.extend(clusters.pop(j))\n",
    "            supergroup.extend(match)\n",
    "            clusters.append(supergroup)\n",
    "            print(clusters)\n",
    "    \n",
    "    # turn into a list of sets to get unique values\n",
    "    clusters = [set(x) for x in clusters] \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f20615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emergency meeting: go over every duplicated name\n",
    "true_people = []\n",
    "duplicates = authors[authors.duplicated(subset=[\"truncatedName\"], keep=False)]\n",
    "\n",
    "for name in set(duplicates[\"truncatedName\"]):\n",
    "    # get all trund names that are exact string matches to this name\n",
    "    #same_names = duplicates[duplicates[\"author_display_name\"]==name]\n",
    "    same_names = duplicates[duplicates[\"truncatedName\"]==name]\n",
    "    matches = []\n",
    "    \n",
    "    for person_a in same_names.itertuples():\n",
    "        aliases = [person_a.Index,]\n",
    "        \n",
    "        for person_b in same_names.itertuples():\n",
    "            if match(person_a, person_b) and person_a.Index!=person_b.Index:\n",
    "                aliases.append(person_b.Index)\n",
    "                \n",
    "        matches.append(aliases)\n",
    "\n",
    "    true_people.extend(cluster(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b509e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all duplicated authors (keep=False)\n",
    "duplicates = authors[authors.duplicated(subset=\"author_display_name\", keep=False)]\n",
    "#singles = authors.drop_duplicates(subset=\"author_display_name\", keep=False)\n",
    "true_people = []\n",
    "\n",
    "# emergency meeting: go over every duplicated name\n",
    "for name in set(duplicates[\"author_display_name\"]):\n",
    "    # get all names that are exact string matches\n",
    "    same_names = duplicates[duplicates[\"author_display_name\"]==name]\n",
    "    \n",
    "    for person in same_names.itertuples():\n",
    "        # check institution first\n",
    "        # this way, even authors without known expertise can be disambiguated first\n",
    "        person_ids = [person.Index,]\n",
    "        orders = person.order.copy()\n",
    "        institutions = [person.inst_id,]\n",
    "        \n",
    "        for cousin in same_names.itertuples():\n",
    "            # if they work at the same institution, they're probably the same person\n",
    "            if cousin.inst_id in institutions:\n",
    "                person_ids.append(cousin.Index)\n",
    "                orders.extend(cousin.order)\n",
    "                institutions.append(cousin.inst_id)\n",
    "                \n",
    "                # try if anyone else looks like this person\n",
    "                for second_cousin in same_names.itertuples():\n",
    "                    if second_cousin.inst_id in institutions:\n",
    "                        person_ids.append(second_cousin.Index)\n",
    "                        orders.extend(second_cousin.order)    \n",
    "                        institutions.append(second_cousin.inst_id)\n",
    "                        # we could go on...\n",
    "        \"\"\"\n",
    "        # check institutions and taxonomic expertise second, simultaneously\n",
    "        if person.species_subject != list():\n",
    "            for cousin in same_names.itertuples():\n",
    "                if len(set(orders).intersection(set(cousin.order))) > 0 or cousin.inst_id in institutions:\n",
    "                    person_ids.add(cousin.Index)\n",
    "                    orders.update(cousin.order)\n",
    "                    institutions.add(cousin.inst_id)\n",
    "                    # try if anyone else looks like this person\n",
    "                    for second_cousin in same_names.itertuples():\n",
    "                        if len(orders.intersection(second_cousin.order)) > 0 \\\n",
    "    or len(institutions.intersection(set(second_cousin.inst_id))) > 0:\n",
    "                            person_ids.add(second_cousin.Index)\n",
    "                            orders.update(second_cousin.order)\n",
    "                            institutions.add(second_cousin.inst_id)\n",
    "                            # we could go on...\n",
    "        \"\"\"\n",
    "        person_ids = set(person_ids)\n",
    "        if person_ids not in true_people:\n",
    "            true_people.append(person_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a642aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_authors = authors[-(authors.duplicated(subset=\"author_display_name\", keep=False))].reset_index()\n",
    "merged_people = []\n",
    "m = 1\n",
    "\n",
    "def collect_values(df, person_ids, column):\n",
    "    if len(person_ids) > 0:\n",
    "        values = []\n",
    "        \n",
    "        for duplicate in person_ids:\n",
    "            imposter = df.loc[duplicate]\n",
    "            if imposter[column] not in values and imposter[column] != None:\n",
    "                if column in levels or column == \"species_subject\":\n",
    "                    values.extend(imposter[column])\n",
    "                else:\n",
    "                    values.append(imposter[column])\n",
    "                \n",
    "    else:\n",
    "        values = df.loc[person_ids, column]\n",
    "\n",
    "    values = set(values)\n",
    "    if len(values)==1:\n",
    "        values = list(values)[0]\n",
    "        \n",
    "    return values\n",
    "\n",
    "person_ids = []\n",
    "m = 1\n",
    "\n",
    "for person in true_people:\n",
    "    row = [person,]\n",
    "    for column in true_authors.columns[1:]:\n",
    "        answer = collect_values(authors, person, column)\n",
    "        row.append(answer)\n",
    "        \n",
    "    merged_people.append(authors.loc[list(person)].reset_index())\n",
    "    merged_people[-1][\"groupNr\"] = m\n",
    "    m += 1\n",
    "    \n",
    "    true_authors.loc[len(true_authors)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a05198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melanie\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:576: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.concat(merged_people, ignore_index=True)        \n",
    "merged_df.to_csv(\"../../data/interim/merged_people.csv\")\n",
    "\n",
    "true_authors.to_pickle(\"../../data/processed/european_authors_disambiguated.pkl\")\n",
    "true_authors.to_csv(\"../../data/processed/european_authors_disambiguated.tsv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
